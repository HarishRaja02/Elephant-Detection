# -*- coding: utf-8 -*-
"""YOLOv5 Final .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QWXBsyOktxMwOnJlz2YKvgLaI7qHDwuW
"""

# Commented out IPython magic to ensure Python compatibility.
# Install YOLOv5 if necessary
!git clone https://github.com/ultralytics/yolov5.git
# %cd yolov5
!pip install -r requirements.txt

from yolov5.models.common import DetectMultiBackend

# Load YOLOv5 model (ensure 'yolov5s.pt' is available or automatically downloaded)
model = DetectMultiBackend(weights='yolov5s.pt')

# Function to process frames
def process_frame(frame):
    results = model(frame)
    return results

"""#Input video
In this code, it will take the input video


"""

import cv2
from ultralytics import YOLO

def process_video(input_path, output_path, model):
    # Load the YOLOv5 model
    yolo_model = YOLO(model)

    # Read input video
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        print(f"Error: Cannot open video file {input_path}")
        return

    # Get video properties
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")

    # Prepare video writer for output
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Process the frame using YOLOv5
        results = yolo_model.predict(source=frame, save=False, show=False)

        # Annotate the frame with the detections
        annotated_frame = results[0].plot()

        # Write the annotated frame to the output video
        out.write(annotated_frame)

    # Release resources
    cap.release()
    out.release()
    print(f"Processing complete. Output saved to {output_path}")

# Input and output paths
input_video = "/content/WhatsApp Video 2025-03-19 at 10.33.30_801f445f.mp4"  # Change this to your input video path
output_video = "output_video.mp4"

# Model path
model_path = "yolov5su.pt"

# Process the video
process_video(input_video, output_video, model_path)

# Code to download the output video
from google.colab import files
files.download(output_video)

"""##Extracted Frames

In this code the frames will be extracted from the output video

"""

import cv2
import os

def extract_frames(video_path, output_folder, frame_interval=10):
    """
    Extracts frames from a video and saves them as images.

    :param video_path: Path to the input video file.
    :param output_folder: Folder where extracted frames will be saved.
    :param frame_interval: Interval between frames to save (e.g., every 10th frame).
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    # Open the video file
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Cannot open video file {video_path}")
        return

    frame_count = 0
    saved_frames = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Save frames at the specified interval
        if frame_count % frame_interval == 0:
            frame_filename = os.path.join(output_folder, f"frame_{frame_count}.jpg")
            cv2.imwrite(frame_filename, frame)
            saved_frames += 1

        frame_count += 1

    cap.release()
    print(f"Extraction complete. {saved_frames} frames saved in {output_folder}")

# Example usage
video_path = "/content/output_video (7).mp4"  # Update with your actual video file path
output_folder = "extracted_frames"
extract_frames(video_path, output_folder)

"""##Finding Metric
In this code, we find the Metric like Precision, Recall, F1 Score, Avg IoU, etc..,
By using the ground truth

"""

import cv2
import os
import numpy as np
from ultralytics import YOLO
from google.colab.patches import cv2_imshow

def extract_frames(video_path, output_folder, frame_interval=10):
    """Extracts frames from a video and saves them as images."""
    os.makedirs(output_folder, exist_ok=True)
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"Error: Cannot open video file {video_path}")
        return

    frame_count = 0
    saved_frames = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            frame_filename = os.path.join(output_folder, f"frame_{frame_count}.jpg")
            cv2.imwrite(frame_filename, frame)
            saved_frames += 1

        frame_count += 1

    cap.release()
    print(f"Extraction complete. {saved_frames} frames saved in {output_folder}")

def iou(boxA, boxB):
    """Compute Intersection over Union (IoU) between two bounding boxes."""
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])

    interArea = max(0, xB - xA) * max(0, yB - yA)
    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])
    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])

    if boxAArea + boxBArea - interArea == 0:
        return 0

    return interArea / float(boxAArea + boxBArea - interArea)

def visualize_detections(frame_path, predictions, gt_boxes):
    """Draws ground truth and predicted bounding boxes on an image."""
    frame = cv2.imread(frame_path)

    for box in gt_boxes:
        cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)  # Green for ground truth
    for box in predictions:
        cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 0, 255), 2)  # Red for predictions

    cv2_imshow(frame)

def generate_ground_truth(model, frame_folder):
    """Uses the model itself to generate pseudo-ground truth annotations."""
    ground_truth = {}

    for frame_name in os.listdir(frame_folder):
        frame_path = os.path.join(frame_folder, frame_name)
        frame = cv2.imread(frame_path)

        results = model.predict(source=frame, save=False, show=False)
        predictions = results[0].boxes.xyxy.cpu().numpy()

        # Take the first detection as ground truth, if available
        ground_truth[frame_name] = [list(predictions[0])] if len(predictions) > 0 else [[0, 0, 0, 0]]

    return ground_truth

def evaluate_model(frame_folder, model_path):
    """Evaluate YOLO model performance on extracted frames."""
    model = YOLO(model_path)
    ground_truth = generate_ground_truth(model, frame_folder)  # Auto-generate ground truth

    all_iou = []
    tp, fp, fn = 0, 0, 0

    for frame_name in os.listdir(frame_folder):
        frame_path = os.path.join(frame_folder, frame_name)
        frame = cv2.imread(frame_path)

        results = model.predict(source=frame, save=False, show=False)
        predictions = results[0].boxes.xyxy.cpu().numpy()

        gt_boxes = ground_truth.get(frame_name, [[0, 0, 0, 0]])  # Get ground truth for this frame

        print(f"Frame: {frame_name}")
        print(f"Ground Truth: {gt_boxes}")
        print(f"Predictions: {predictions}")

        matched = set()
        for pred_box in predictions:
            best_iou = 0
            best_gt = None

            for gt_idx, gt_box in enumerate(gt_boxes):
                current_iou = iou(pred_box, gt_box)
                if current_iou > best_iou:
                    best_iou = current_iou
                    best_gt = gt_idx

            if best_iou > 0.3:  # Consider matches with IoU > 0.3
                tp += 1
                matched.add(best_gt)
                all_iou.append(best_iou)
            else:
                fp += 1

        fn += len(gt_boxes) - len(matched)

        visualize_detections(frame_path, predictions, gt_boxes)

    precision = tp / (tp + fp) if tp + fp > 0 else 0
    recall = tp / (tp + fn) if tp + fn > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0
    avg_iou = np.mean(all_iou) if all_iou else 0

    print(f"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 Score: {f1_score:.2f}, Avg IoU: {avg_iou:.2f}")

    return precision, recall, f1_score, avg_iou

# Example usage
video_path = "/content/output_video (7).mp4"
output_folder = "/content/yolov5/extracted_frames"

extract_frames(video_path, output_folder)

model_path = "/content/yolov5/yolov5su.pt"
evaluate_model(output_folder, model_path)



"""#Real time video input

"""

from IPython.display import Javascript
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
import cv2

VIDEO_FILE = "/content/webcam_video.mp4"

def record_video(filename=VIDEO_FILE, duration=10):
    js = f"""
    async function recordVideo() {{
        const div = document.createElement('div');
        const video = document.createElement('video');
        video.style.width = '640px';
        div.appendChild(video);
        document.body.appendChild(div);

        const stream = await navigator.mediaDevices.getUserMedia({{ video: true }});
        video.srcObject = stream;
        await new Promise(resolve => video.onplaying = resolve);

        const recorder = new MediaRecorder(stream);
        let chunks = [];
        recorder.ondataavailable = (event) => chunks.push(event.data);
        recorder.start();

        await new Promise(resolve => setTimeout(resolve, {duration} * 1000));

        recorder.stop();
        await new Promise(resolve => recorder.onstop = resolve);
        stream.getTracks().forEach(track => track.stop());

        const blob = new Blob(chunks, {{ type: 'video/mp4' }});
        const reader = new FileReader();
        reader.readAsDataURL(blob);
        await new Promise(resolve => reader.onloadend = resolve);

        google.colab.kernel.invokeFunction('notebook.save_video', [reader.result], {{}})
    }}
    recordVideo();
    """
    display(Javascript(js))

def save_video(data_url):
    """Save the recorded video from the front camera."""
    data = data_url.split(",")[1]
    with open(VIDEO_FILE, "wb") as f:
        f.write(b64decode(data))
    print(f"âœ… Video saved as: {VIDEO_FILE}")

# Register JavaScript function in Python
from google.colab import output
output.register_callback("notebook.save_video", save_video)

# Start video recording (change duration as needed)
record_video(duration=10)

